{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60683f8c",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdaf78e",
   "metadata": {},
   "source": [
    "##### Task\n",
    "1. Exploring the Amazon Sales Dataset involves a step-by-step process. First, we clean and prepare the data to ensure it's accurate and consistent. Then, we summarize the data using descriptive statistics like averages and ranges. Next, we visualize the data with charts and graphs to see patterns and relationships. We detect outliers, which are unusual data points, and test our assumptions about the data. We divide the data into groups for better understanding and finally, we summarize our findings.\n",
    "\n",
    "##### 2. Objectives\n",
    "    a) The primary objective of analyzing the Amazon Sales Dataset is delve into product categories, prices, ratings, and sales patterns to identify characteristics that resonate with consumers and propel them to purchase.\n",
    "\n",
    "    b) Delve into product categories, prices, ratings, and sales patterns to identify characteristics that resonate with consumers and propel them to purchase.\n",
    "\n",
    "    c) Translate insights into actionable recommendations that optimize product development, inform marketing strategies, and boost your competitive edge.\n",
    "\n",
    "    d) Equip businesses with the knowledge to create products that cater to evolving consumer needs and desires.\n",
    "\n",
    "    e) Craft communication strategies that resonate with specific demographics and maximize engagement.\n",
    "\n",
    "    f) Facilitate a marketplace where products find their perfect match in the hearts of consumers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b945dbb8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8de64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy as sp\n",
    "# this is for jupyter notebook to show the plot in the notebook itself instead of opening a new window\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcfc96c",
   "metadata": {},
   "source": [
    "#### LOADING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c76d1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./amazon_data/amazon.csv')\n",
    "pd.set_option('display.max_columns', None) # to display all columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bad8cba",
   "metadata": {},
   "source": [
    "#### BASIC ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfc7e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5) # See first 5 rows of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecfab75",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns # See column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8362ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The Number of Rows are {df.shape[0]}, and columns are {df.shape[1]}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526e593c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info() # Get info about data types and non-null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535698dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28996d8d",
   "metadata": {},
   "source": [
    "### ASSUMPTION:\n",
    "\n",
    "There are 1465 rows and 16 columns in the dataset.\n",
    "\n",
    "The data type of all columns is object.\n",
    "\n",
    "The columns in the datasets are:\n",
    "'product_id', 'product_name', 'category', 'discounted_price', 'actual_price', 'discount_percentage', 'rating', 'rating_count', 'about_product', 'user_id', 'user_name', 'review_id', 'review_title', 'review_content', 'img_link', 'product_link'\n",
    "\n",
    "There are a few missing values in the dataset, which we will read in detail and deal with later on in the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6e3b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the data type of discounted price and actual price\n",
    "\n",
    "df['discounted_price'] = df['discounted_price'].str.replace(\"₹\",'')\n",
    "df['discounted_price'] = df['discounted_price'].str.replace(\",\",'')\n",
    "df['discounted_price'] = df['discounted_price'].astype('float64')\n",
    "\n",
    "df['actual_price'] = df['actual_price'].str.replace(\"₹\",'')\n",
    "df['actual_price'] = df['actual_price'].str.replace(\",\",'')\n",
    "df['actual_price'] = df['actual_price'].astype('float64')\n",
    "\n",
    "# Changing Datatype and values in Discount Percentage\n",
    "\n",
    "df['discount_percentage'] = df['discount_percentage'].str.replace('%','').astype('float64')\n",
    "df['discount_percentage'] = df['discount_percentage'] / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c71020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding unusual string in rating column\n",
    "df['rating'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacf8734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the strange row\n",
    "df.query('rating == \"|\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61899aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing Rating Columns Data Type\n",
    "\n",
    "df['rating'] = df['rating'].str.replace('|', '3.9').astype('float64')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0404a337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing 'rating_count' Column Data Type\n",
    "\n",
    "df['rating_count'] = df['rating_count'].str.replace(',', '').astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9e9664",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info() # Get a concise summary of the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e0b661",
   "metadata": {},
   "source": [
    "### DESCRIPTIVE STATISTICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe360fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe() # Get descriptive statistics for numerical columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5dc583",
   "metadata": {},
   "source": [
    "ASSUMPTION\n",
    "\n",
    "All columns data type was object So, I converted some column data type to float.\n",
    "\n",
    "There are 4 numeric as per Python coding or descriptive statistics from Python describe function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca95aba",
   "metadata": {},
   "source": [
    "### MISSING VALUES\n",
    "\n",
    "Dealing with the missing values is one of the most important part of the data wrangling process, we must deal with the missing values in order to get the correct insights from the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411ba2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3479c1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find missing values percentage in the data\n",
    "round(df.isnull().sum() / len(df) * 100, 2).sort_values(ascending=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a2e617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find total number of missing values\n",
    "df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31268326",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# make a figure size\n",
    "plt.figure(figsize=(22, 10))\n",
    "# plot the null values in each column\n",
    "sns.heatmap(df.isnull(), yticklabels=False, cbar=False, cmap='viridis') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f810d640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make figure size\n",
    "plt.figure(figsize=(22, 10))\n",
    "# plot the null values by their percentage in each column\n",
    "missing_percentage = df.isnull().sum()/len(df)*100\n",
    "missing_percentage.plot(kind='bar')\n",
    "# add the labels\n",
    "plt.xlabel('Columns')\n",
    "plt.ylabel('Percentage')\n",
    "plt.title('Percentage of Missing Values in each Column')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c780d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are only viewing the rows where there are null values in the column.\n",
    "\n",
    "df[df['rating_count'].isnull()].head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443a481b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values\n",
    "df['rating_count'] = df.rating_count.fillna(value=df['rating_count'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b81703",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259aa436",
   "metadata": {},
   "source": [
    "### DUPLICATED VALUES\n",
    "\n",
    "Removing duplicates is one of the most important part of the data wrangling process, we must remove the duplicates in order to get the correct insights from the data.\n",
    "\n",
    "If you do not remove duplicates from a dataset, it can lead to incorrect insights and analysis.\n",
    "\n",
    "Duplicates can skew statistical measures such as mean, median, and standard deviation, and can also lead to over-representation of certain data points.\n",
    "\n",
    "It is important to remove duplicates to ensure the accuracy and reliability of your data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea7ec37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Duplicate\n",
    "df.duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65f0b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594972fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "any_duplicates = df.duplicated(subset=['product_id', 'product_name', 'category', 'discounted_price',\n",
    "       'actual_price', 'discount_percentage', 'rating', 'rating_count',\n",
    "       'about_product', 'user_id', 'user_name', 'review_id', 'review_title',\n",
    "       'review_content', 'img_link', 'product_link']).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc791730",
   "metadata": {},
   "outputs": [],
   "source": [
    "any_duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f805df9b",
   "metadata": {},
   "source": [
    "### DATA VISUALIZATION\n",
    "\n",
    "Hence all duplicates are done we do data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ae7fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dont show warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# SCATTER PLOT\n",
    "\n",
    "# Plot actual_price vs. rating\n",
    "plt.scatter(df['actual_price'], df['rating'])\n",
    "plt.xlabel('Actual_price')\n",
    "plt.ylabel('Rating')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37f4240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HISTOGRAM\n",
    "\n",
    "# Plot distribution of actual_price\n",
    "plt.hist(df['actual_price'])\n",
    "plt.xlabel('Actual Price')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da6bfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "# label encode categorical variables\n",
    "# Label encoding is a technique used to convert categorical text data into numerical values. \n",
    "# It assigns each unique category in a column an integer value, which allows machine learning algorithms to process the data.\n",
    "# LabelEncoder from sklearn is used for this purpose. It fits on the unique values of a column and transforms each value to an integer label.\n",
    "\n",
    "le_product_id = LabelEncoder()\n",
    "le_category = LabelEncoder()\n",
    "le_review_id = LabelEncoder()\n",
    "le_review_content = LabelEncoder()\n",
    "le_product_name = LabelEncoder()\n",
    "le_user_name = LabelEncoder()\n",
    "le_about_product = LabelEncoder()\n",
    "le_user_id = LabelEncoder()\n",
    "le_review_title = LabelEncoder()\n",
    "le_img_link = LabelEncoder()\n",
    "le_product_link = LabelEncoder()\n",
    "\n",
    "\n",
    "df['product_id'] = le_product_id.fit_transform(df['product_id'])\n",
    "df['category'] = le_category.fit_transform(df['category'])\n",
    "df['review_id'] = le_review_id.fit_transform(df['review_id'])\n",
    "df['review_content'] = le_review_content.fit_transform(df['review_content'])\n",
    "df['product_name'] = le_product_name.fit_transform(df['product_name'])\n",
    "df['user_name'] = le_user_name.fit_transform(df['user_name'])\n",
    "df['about_product'] = le_about_product.fit_transform(df['about_product'])\n",
    "df['user_id'] = le_user_id.fit_transform(df['user_id'])\n",
    "df['review_title'] = le_review_title.fit_transform(df['review_title'])\n",
    "df['img_link'] = le_img_link.fit_transform(df['img_link'])\n",
    "df['product_link'] = le_product_link.fit_transform(df['product_link'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d27c477",
   "metadata": {},
   "source": [
    "### Correlation Analysis\n",
    "Correlation measures the strength and direction of a linear relationship between two variables. \n",
    "\n",
    "Correlation analysis is a statistical technique used to evaluate how closely related two or more variables are. \n",
    "\n",
    "The correlation coefficient ranges from -1 to 1, where values close to 1 indicate a strong positive relationship, values close to -1 indicate a strong negative relationship, and values near 0 suggest no linear relationship. \n",
    "\n",
    "In data analysis, correlation helps identify patterns, dependencies, and potential predictive relationships between variables, guiding further exploration and modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da526dc0",
   "metadata": {},
   "source": [
    "### How to Read Heatmaps\n",
    "\n",
    "A heatmap is a graphical representation of data where individual values are represented as colors. In the context of correlation matrices:\n",
    "\n",
    "- **Axes**: Both the x-axis and y-axis list the variables being compared.\n",
    "- **Cells**: Each cell shows the correlation coefficient between the pair of variables at that row and column.\n",
    "- **Color Intensity**: The color indicates the strength and direction of the correlation:\n",
    "    - **Dark/Intense Colors** (e.g., deep red or blue): Strong correlation (positive or negative).\n",
    "    - **Light Colors**: Weak or no correlation.\n",
    "- **Annotations**: Numbers inside the cells show the exact correlation coefficient (e.g., 0.85, -0.45).\n",
    "- **Diagonal**: The diagonal from top-left to bottom-right always shows a perfect correlation (value = 1) since each variable is perfectly correlated with itself.\n",
    "- **Masking**: Sometimes, the upper or lower triangle is masked to avoid redundancy, as the matrix is symmetric.\n",
    "\n",
    "**Interpretation Example:**\n",
    "- A cell with a value close to **1** (and a strong color) means a strong positive relationship.\n",
    "- A cell with a value close to **-1** (and a strong color in the opposite direction) means a strong negative relationship.\n",
    "- A cell with a value near **0** (and a neutral color) means little or no linear relationship.\n",
    "\n",
    "Heatmaps help quickly identify which variables are strongly or weakly related, guiding further analysis or feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69fc9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Pearson correlation coefficients (default in Pandas)\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "# Print the correlation matrix\n",
    "print(correlation_matrix)\n",
    "\n",
    "# Create a heatmap to visualize the correlations\n",
    "plt.figure(figsize=(14, 10))\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "sns.heatmap(\n",
    "    correlation_matrix,\n",
    "    annot=True,\n",
    "    cmap=\"coolwarm\",\n",
    "    fmt=\".2f\",\n",
    "    mask=mask,\n",
    "    linewidths=0.5,\n",
    "    cbar_kws={\"shrink\": 0.8, \"label\": \"Pearson Correlation\"}\n",
    ")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.title(\"Correlation Matrix (Pearson)\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate Spearman correlation coefficients (for non-linear relationships)\n",
    "spearman_correlation_matrix = df.corr(method=\"spearman\")\n",
    "\n",
    "# Print the Spearman correlation matrix\n",
    "\n",
    "# Improve the heatmap: larger figure, better colorbar, rotated labels, mask upper triangle for clarity\n",
    "plt.figure(figsize=(14, 10))\n",
    "mask = np.triu(np.ones_like(spearman_correlation_matrix, dtype=bool))\n",
    "sns.heatmap(\n",
    "    spearman_correlation_matrix, \n",
    "    annot=True, \n",
    "    cmap=\"coolwarm\", \n",
    "    fmt=\".2f\", \n",
    "    mask=mask,\n",
    "    linewidths=0.5, \n",
    "    cbar_kws={\"shrink\": 0.8, \"label\": \"Spearman Correlation\"}\n",
    ")\n",
    "plt.title(\"Correlation Matrix (Spearman)\", fontsize=16)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(spearman_correlation_matrix)\n",
    "\n",
    "# # Create a heatmap to visualize the Spearman correlations\n",
    "# sns.heatmap(spearman_correlation_matrix, annot=True, cmap=\"coolwarm\")\n",
    "# plt.title(\"Correlation Matrix (Spearman)\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f3685d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation coefficient between product price and sales\n",
    "correlation_coefficient = np.corrcoef(df['actual_price'], df['rating'])[0, 1]\n",
    "\n",
    "# Print correlation coefficient\n",
    "print(correlation_coefficient)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44783831",
   "metadata": {},
   "source": [
    "GROUPING AND AGGREGATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435d735a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean sales by product category\n",
    "grouped_df = df.groupby('category')['rating'].mean()\n",
    "\n",
    "# Print mean sales by product category\n",
    "print(grouped_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e44908d",
   "metadata": {},
   "source": [
    "CALCULATE SUMMARY STATISTICS FOR GROUPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59b0244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean rating by category\n",
    "mean_sales_by_category = df.groupby('category')['rating'].mean()\n",
    "print(mean_sales_by_category)\n",
    "\n",
    "# Median rating by review_content\n",
    "median_sales_by_age = df.groupby('review_content')['rating'].median()\n",
    "print(median_sales_by_age)\n",
    "\n",
    "# Standard deviation of actual_price by product_name\n",
    "std_price_by_brand = df.groupby('product_name')['actual_price'].std()\n",
    "print(std_price_by_brand)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4a2901",
   "metadata": {},
   "source": [
    "CREATE PIVOT TABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38a6039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot table of rating by category and customer location\n",
    "pivot_table = df.pivot_table(values='rating', index='category', columns='product_link', aggfunc='mean')\n",
    "print(pivot_table)\n",
    "\n",
    "# Pivot table of average rating_count by customer age group and product category\n",
    "pivot_table = df.pivot_table(values='rating_count', index='review_content', columns='category', aggfunc='mean')\n",
    "print(pivot_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebe3aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverse transform the data\n",
    "# Inverse transforming with LabelEncoder reverses the label encoding process, converting numerical labels back to their original categorical string values.\n",
    "# This is useful when you want to interpret or present results in their original, human-readable form after processing or modeling.\n",
    "\n",
    "# How to read and interpret:\n",
    "# - After inverse transformation, columns that were previously encoded as integers will now display their original string categories.\n",
    "# - For example, if 'category' was encoded as 0, 1, 2 for 'Electronics', 'Clothing', 'Books', after inverse_transform, those numbers revert to their original names.\n",
    "# - This makes the data easier to understand and analyze, especially for reporting or visualization.\n",
    "\n",
    "df['product_id'] = le_product_id.inverse_transform(df['product_id'])\n",
    "df['category'] = le_category.inverse_transform(df['category'])\n",
    "df['review_id'] = le_review_id.inverse_transform(df['review_id'])\n",
    "df['review_content'] = le_review_content.inverse_transform(df['review_content'])\n",
    "df['product_name'] = le_product_name.inverse_transform(df['product_name'])\n",
    "df['user_name'] = le_user_name.inverse_transform(df['user_name'])\n",
    "\n",
    "df['user_id'] = le_user_id.inverse_transform(df['user_id'])\n",
    "df['review_title'] = le_review_title.inverse_transform(df['review_title'])\n",
    "df['img_link'] = le_img_link.inverse_transform(df['img_link'])\n",
    "df['product_link'] = le_product_link.inverse_transform(df['product_link'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
